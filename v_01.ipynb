{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e104b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 시작 전 실행하기\n",
    "# D 드라이브로 변경하는 코드\n",
    "import os\n",
    "\n",
    "# Hugging Face 캐시 경로 변경\n",
    "os.environ['HF_HOME'] = r'D:\\huggingface_cache'          # 캐시 전체 경로\n",
    "os.environ['TRANSFORMERS_CACHE'] = r'D:\\huggingface_cache\\transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = r'D:\\huggingface_cache\\datasets'\n",
    "os.environ['HF_METRICS_CACHE'] = r'D:\\huggingface_cache\\metrics'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d35c73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 코드 버전 확인용\n",
    "# 버전 확인 하고 가상환경이 맞는지 확인하기\n",
    "import torch\n",
    "print(torch.__version__)      # PyTorch 버전\n",
    "print(torch.version.cuda)     # CUDA 버전\n",
    "print(torch.cuda.is_available())  # GPU 사용 가능 여부\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3fbcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a giraffe\n"
     ]
    }
   ],
   "source": [
    "# 1️ 필요한 라이브러리\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# 2️ GPU 사용 여부 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 3️ 모델 및 프로세서 로드\n",
    "model_name = \"Salesforce/blip2-flan-t5-xl\"  # Windows + GPU 호환 안정 모델\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 4️ 이미지 불러오기\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n",
    "img = Image.open(BytesIO(requests.get(image_url).content)).convert(\"RGB\")\n",
    "\n",
    "# 5️ 텍스트 질문\n",
    "question = \"What animal is on the candy?\"\n",
    "\n",
    "# 6️ 입력 처리\n",
    "inputs = processor(images=img, text=question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 7️ 모델 실행\n",
    "generated_ids = model.generate(**inputs)\n",
    "answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# 8️ 결과 출력\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8234ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# 한국어 질문 예시\n",
    "img_url = \"https://www.gyeongju.go.kr/upload/content/thumb/gyimage/%EC%B2%A8%EC%84%B1%EB%8C%80%EC%9D%98%20%EC%95%84%EB%A6%84%EB%8B%A4%EC%9B%80_1_.jpg\"\n",
    "img = Image.open(BytesIO(requests.get(img_url).content)).convert(\"RGB\")\n",
    "question = \"introduce this image in korean\"\n",
    "\n",
    "# 입력 처리\n",
    "inputs = processor(images=img, text=question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델 실행\n",
    "generated_ids = model.generate(**inputs)\n",
    "answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cde02af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Incorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 5. HuggingFace pipeline에서 이미지+질문 직접 실행\u001b[39;00m\n\u001b[0;32m     34\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat animal is on the candy?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m raw_answer \u001b[38;5;241m=\u001b[39m \u001b[43mblip2_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 6. LangChain으로 후처리 (텍스트만 전달)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m result \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe image was asked: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The raw answer was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Summarize clearly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:158\u001b[0m, in \u001b[0;36mImageToTextPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call the image-to-text pipeline without an inputs argument!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\base.py:1458\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1451\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1452\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1455\u001b[0m         )\n\u001b[0;32m   1456\u001b[0m     )\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\base.py:1464\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1464\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1465\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1466\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:161\u001b[0m, in \u001b[0;36mImageToTextPipeline.preprocess\u001b[1;34m(self, image, prompt, timeout)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 161\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    165\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `prompt` to the `image-to-text` pipeline is deprecated and will be removed in version 4.48\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of 🤗 Transformers. Use the `image-text-to-text` pipeline instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    167\u001b[0m         )\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\image_utils.py:496\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image, timeout)\u001b[0m\n\u001b[0;32m    494\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    499\u001b[0m image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImageOps\u001b[38;5;241m.\u001b[39mexif_transpose(image)\n\u001b[0;32m    500\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Incorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image."
     ]
    }
   ],
   "source": [
    "# 랭체인 사용하는 코드\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, pipeline\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "from io import BytesIO\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# 1. 모델 및 프로세서 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Salesforce/blip2-flan-t5-xl\"   \n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# 2. HuggingFace pipeline 생성\n",
    "blip2_pipe = pipeline(\n",
    "    \"image-to-text\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    image_processor=processor.image_processor,\n",
    "    device=0 if device==\"cuda\" else -1\n",
    ")\n",
    "\n",
    "# 3. LangChain LLM 래퍼로 감싸기 (텍스트 전용)\n",
    "llm = HuggingFacePipeline(pipeline=blip2_pipe)\n",
    "\n",
    "# 4. 이미지 불러오기\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n",
    "img = Image.open(BytesIO(requests.get(image_url).content)).convert(\"RGB\")\n",
    "\n",
    "# 5. HuggingFace pipeline에서 이미지+질문 직접 실행\n",
    "question = \"What animal is on the candy?\"\n",
    "raw_answer = blip2_pipe({\"image\": img, \"text\": question})[0][\"generated_text\"]\n",
    "\n",
    "# 6. LangChain으로 후처리 (텍스트만 전달)\n",
    "result = llm.invoke(f\"The image was asked: {question}. The raw answer was: {raw_answer}. Summarize clearly.\")\n",
    "\n",
    "print(\"Answer:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af0f1c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]\n",
      "Device set to use cuda:0\n",
      "Passing `prompt` to the `image-to-text` pipeline is deprecated and will be removed in version 4.48 of 🤗 Transformers. Use the `image-text-to-text` pipeline instead\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got The image was asked: 'What animal is on the candy?'. The raw answer from BLIP2 was: 'What animal is on the candy?\n'. Summarize it clearly.. Failed with Incorrect padding",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\image_utils.py:487\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image, timeout)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 487\u001b[0m     b64 \u001b[38;5;241m=\u001b[39m \u001b[43mbase64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodebytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(b64))\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\base64.py:562\u001b[0m, in \u001b[0;36mdecodebytes\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    561\u001b[0m _input_type_check(s)\n\u001b[1;32m--> 562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinascii\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma2b_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mError\u001b[0m: Incorrect padding",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m raw_answer \u001b[38;5;241m=\u001b[39m blip2_pipe(img, prompt\u001b[38;5;241m=\u001b[39mquestion)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 7. LangChain에서 텍스트 후처리 (선택적)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe image was asked: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m. The raw answer from BLIP2 was: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mraw_answer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m. Summarize it clearly.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     45\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_answer)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    387\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 389\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    390\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    391\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    392\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    395\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    396\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    397\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    401\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    764\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    765\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    970\u001b[0m     ]\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    972\u001b[0m         prompts,\n\u001b[0;32m    973\u001b[0m         stop,\n\u001b[0;32m    974\u001b[0m         run_managers,\n\u001b[0;32m    975\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    989\u001b[0m     ]\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 792\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    793\u001b[0m                 prompts,\n\u001b[0;32m    794\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    796\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    798\u001b[0m             )\n\u001b[0;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_community\\llms\\huggingface_pipeline.py:285\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[0;32m    286\u001b[0m     batch_prompts,\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpipeline_kwargs,\n\u001b[0;32m    288\u001b[0m )\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:158\u001b[0m, in \u001b[0;36mImageToTextPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call the image-to-text pipeline without an inputs argument!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\base.py:1439\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1436\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1437\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1438\u001b[0m     )\n\u001b[1;32m-> 1439\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:19\u001b[0m, in \u001b[0;36mPipelineDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m     18\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[i]\n\u001b[1;32m---> 19\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:161\u001b[0m, in \u001b[0;36mImageToTextPipeline.preprocess\u001b[1;34m(self, image, prompt, timeout)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 161\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    165\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `prompt` to the `image-to-text` pipeline is deprecated and will be removed in version 4.48\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of 🤗 Transformers. Use the `image-text-to-text` pipeline instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    167\u001b[0m         )\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\transformers\\image_utils.py:490\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image, timeout)\u001b[0m\n\u001b[0;32m    488\u001b[0m             image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(b64))\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 490\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    491\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Failed with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m             )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    494\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\n",
      "\u001b[1;31mValueError\u001b[0m: Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got The image was asked: 'What animal is on the candy?'. The raw answer from BLIP2 was: 'What animal is on the candy?\n'. Summarize it clearly.. Failed with Incorrect padding"
     ]
    }
   ],
   "source": [
    "# 0. 필요 라이브러리\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, pipeline\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "from io import BytesIO\n",
    "from langchain_community.llms import HuggingFacePipeline   # 최신 LangChain LLM 래퍼\n",
    "\n",
    "# 1. 디바이스 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. 모델 및 프로세서 로드\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"  # GPU 8GB 환경에서 안정적\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "\n",
    "# 메모리 절약 옵션: float16 + device_map\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3. HuggingFace pipeline 생성 (이미지->텍스트)\n",
    "blip2_pipe = pipeline(\n",
    "    task=\"image-to-text\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    image_processor=processor.image_processor,\n",
    ")\n",
    "\n",
    "# 4. LangChain LLM 래퍼 (텍스트 후처리용)\n",
    "llm = HuggingFacePipeline(pipeline=blip2_pipe)\n",
    "\n",
    "# 5. 이미지 불러오기\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n",
    "img = Image.open(BytesIO(requests.get(image_url).content)).convert(\"RGB\")\n",
    "\n",
    "# 6. HuggingFace pipeline에서 이미지+질문 처리\n",
    "question = \"What animal is on the candy?\"\n",
    "raw_answer = blip2_pipe(img, prompt=question)[0][\"generated_text\"]\n",
    "\n",
    "# 7. LangChain에서 텍스트 후처리 (선택적)\n",
    "final_answer = llm.invoke(\n",
    "    f\"The image was asked: '{question}'. The raw answer from BLIP2 was: '{raw_answer}'. Summarize it clearly.\"\n",
    ")\n",
    "\n",
    "print(\"Answer:\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3713cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]\n",
      "d:\\conda_envs\\myenv\\lib\\site-packages\\accelerate\\utils\\modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['query_tokens']\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Model file does not exist: WindowsPath('C:/Users/EunSung/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     23\u001b[0m blip2_pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     24\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     26\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m     27\u001b[0m     image_processor\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mimage_processor\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 4. 로컬 LLM (GPT4All 예시)\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mggml-gpt4all-j-v1.3-groovy.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 로컬 모델 파일 필요\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 5. 이미지 불러오기\u001b[39;00m\n\u001b[0;32m     36\u001b[0m image_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\pydantic\\_internal\\_decorators_v1.py:148\u001b[0m, in \u001b[0;36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[1;34m(values, _)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema\u001b[38;5;241m.\u001b[39mValidationInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RootValidatorValues:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_core\\utils\\pydantic.py:168\u001b[0m, in \u001b[0;36mpre_init.<locals>.wrapper\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    165\u001b[0m             values[name] \u001b[38;5;241m=\u001b[39m field_info\u001b[38;5;241m.\u001b[39mdefault\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\langchain_community\\llms\\gpt4all.py:145\u001b[0m, in \u001b[0;36mGPT4All.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    142\u001b[0m model_path, delimiter, model_name \u001b[38;5;241m=\u001b[39m full_path\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m model_path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m delimiter\n\u001b[1;32m--> 145\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4AllModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow_download\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# set n_threads\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_thread_count(values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\gpt4all\\gpt4all.py:235\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[0;32m    232\u001b[0m         device_init \u001b[38;5;241m=\u001b[39m _remove_prefix(device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkompute:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl, backend)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\lib\\site-packages\\gpt4all\\gpt4all.py:346\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[0;32m    341\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_model(\n\u001b[0;32m    342\u001b[0m         model_filename, model_path, verbose\u001b[38;5;241m=\u001b[39mverbose, url\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    343\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m filesize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(filesize), expected_md5\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd5sum\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    344\u001b[0m     ))\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dest\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Model file does not exist: WindowsPath('C:/Users/EunSung/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin')"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, pipeline\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "from io import BytesIO\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 디바이스 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------------\n",
    "# 2. BLIP2 모델 및 프로세서 로드\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"  # 8GB GPU safe\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. 이미지 -> 텍스트 pipeline 생성\n",
    "blip2_pipe = pipeline(\n",
    "    task=\"image-to-text\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    image_processor=processor.image_processor\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 로컬 LLM (GPT4All 예시)\n",
    "llm = GPT4All(model=\"ggml-gpt4all-j-v1.3-groovy.bin\")  # 로컬 모델 파일 필요\n",
    "\n",
    "# -------------------------------\n",
    "# 5. 이미지 불러오기\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n",
    "response = requests.get(image_url)\n",
    "response.raise_for_status()\n",
    "img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. BLIP2로 이미지 처리\n",
    "question = \"What animal is on the candy?\"\n",
    "raw_answer = blip2_pipe(img, prompt=question)[0][\"generated_text\"]\n",
    "print(\"Raw Answer:\", raw_answer)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. 로컬 LLM으로 후처리 (텍스트 요약/정리)\n",
    "final_answer = llm(f\"The raw answer from BLIP2 was: '{raw_answer}'. Summarize clearly.\")\n",
    "print(\"Final Answer:\", final_answer)\n",
    "\n",
    "# ------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9100945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()                   # 파이썬 메모리 정리\n",
    "torch.cuda.empty_cache()        # CUDA 캐시 비우기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c128b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt4all\n",
      "  Downloading gpt4all-2.8.2-py3-none-win_amd64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests in d:\\conda_envs\\myenv\\lib\\site-packages (from gpt4all) (2.32.5)\n",
      "Requirement already satisfied: tqdm in d:\\conda_envs\\myenv\\lib\\site-packages (from gpt4all) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in d:\\conda_envs\\myenv\\lib\\site-packages (from gpt4all) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\conda_envs\\myenv\\lib\\site-packages (from requests->gpt4all) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\conda_envs\\myenv\\lib\\site-packages (from requests->gpt4all) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\conda_envs\\myenv\\lib\\site-packages (from requests->gpt4all) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\conda_envs\\myenv\\lib\\site-packages (from requests->gpt4all) (2025.8.3)\n",
      "Requirement already satisfied: colorama in d:\\conda_envs\\myenv\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n",
      "Downloading gpt4all-2.8.2-py3-none-win_amd64.whl (119.6 MB)\n",
      "   ---------------------------------------- 0.0/119.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/119.6 MB 4.2 MB/s eta 0:00:29\n",
      "    --------------------------------------- 1.6/119.6 MB 3.8 MB/s eta 0:00:31\n",
      "    --------------------------------------- 2.4/119.6 MB 3.7 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 3.1/119.6 MB 3.8 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 4.2/119.6 MB 3.9 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 5.2/119.6 MB 4.2 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 6.6/119.6 MB 4.4 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 7.3/119.6 MB 4.5 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 8.4/119.6 MB 4.5 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 10.0/119.6 MB 4.7 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 11.5/119.6 MB 5.1 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 12.6/119.6 MB 5.1 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 13.6/119.6 MB 5.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 14.9/119.6 MB 5.1 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 16.0/119.6 MB 5.1 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 17.0/119.6 MB 5.1 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 19.1/119.6 MB 5.4 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 20.2/119.6 MB 5.4 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 21.5/119.6 MB 5.4 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 22.8/119.6 MB 5.5 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 23.9/119.6 MB 5.5 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 24.9/119.6 MB 5.5 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 27.0/119.6 MB 5.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 28.0/119.6 MB 5.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 29.1/119.6 MB 5.6 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 30.4/119.6 MB 5.7 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 31.5/119.6 MB 5.6 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 32.8/119.6 MB 5.7 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 35.1/119.6 MB 5.9 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 36.7/119.6 MB 5.9 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 37.7/119.6 MB 5.9 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 39.1/119.6 MB 5.9 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 40.6/119.6 MB 5.9 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 41.4/119.6 MB 5.9 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 43.8/119.6 MB 6.0 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 45.4/119.6 MB 6.1 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 46.7/119.6 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 48.2/119.6 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 49.8/119.6 MB 6.2 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 51.6/119.6 MB 6.3 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 54.0/119.6 MB 6.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 55.8/119.6 MB 6.4 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 57.4/119.6 MB 6.5 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 59.2/119.6 MB 6.5 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 60.8/119.6 MB 6.5 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 61.9/119.6 MB 6.5 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 63.7/119.6 MB 6.6 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 65.5/119.6 MB 6.6 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 67.1/119.6 MB 6.6 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 68.9/119.6 MB 6.7 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 70.5/119.6 MB 6.7 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 72.1/119.6 MB 6.7 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 73.4/119.6 MB 6.7 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 74.2/119.6 MB 6.7 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 75.2/119.6 MB 6.6 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 76.8/119.6 MB 6.6 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 78.9/119.6 MB 6.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 80.7/119.6 MB 6.8 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 81.3/119.6 MB 6.7 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 83.6/119.6 MB 6.8 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 85.5/119.6 MB 6.8 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 87.3/119.6 MB 6.8 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 89.4/119.6 MB 6.9 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 91.5/119.6 MB 6.9 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 93.6/119.6 MB 7.0 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 95.9/119.6 MB 7.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 97.8/119.6 MB 7.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 99.9/119.6 MB 7.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 101.7/119.6 MB 7.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 103.8/119.6 MB 7.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 105.4/119.6 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 107.7/119.6 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 109.6/119.6 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 111.1/119.6 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 112.7/119.6 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 114.6/119.6 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 116.1/119.6 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  118.2/119.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  119.5/119.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 119.6/119.6 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: gpt4all\n",
      "Successfully installed gpt4all-2.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c1249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
